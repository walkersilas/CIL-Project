From \autoref{tab:results} we can observe that RGNN performs significantly better than its baseline counterparts. Models combining graph-extracted information with a feed-forward network, i.e. GNN + NCF and RGNN, perform best in our experiments. This suggests that high-order graph information accurately describes collaborative filtering interactions. Further, we observe that reinforcements improve the performance of the GNN + NCF model. This may be due to the additional information contained in the reinforcements and the final linear layer learning when and how well the user-item pair is representable in the GNN + NCF compared to that of the simple baseline used to generate the reinforcements. As a word of caution: The previous statement is true only when high quality reinforcements are added to the model. Whenever low quality reinforcements, such as those generated by the SVD baseline, are added, the model may overfit to the reinforcements and perform worse (see \autoref{tab:results_rgnn}).

\autoref{tab:results} reports the computation time to run the entire training and prediction process from start to finish. We observe that the neural network models and the SVD++ baseline have the highest time complexity. This may be caused by the neural networks having the largest number of trainable parameters. Conversely, the SVD++ baseline is slow due to not utilizing any fast specialized hardware acceleration such as GPUs. Further, we observe that learning graph embeddings reduces convergence time compared to the standard embeddings in the NCF model.

As already shown by some recent work of Rendle et al. \cite{rendle2019difficulty}, well tuned baselines perform comparably to deep learning models using much less time and resources during the training process. This is also corroborated by our results (see \autoref{tab:results}). The widespread availability of specialized hardware accelerators such as graphics processing units (GPUs) or tensor processing units (TPUs), however, provides wide access to training deep learning architectures with ease like the RGNN proposed in this paper to achieve better performance at the cost of additional training time and resources.